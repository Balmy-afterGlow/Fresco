import os
import time
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset
from torch.amp import autocast, GradScaler
from torch.optim.lr_scheduler import OneCycleLR, CosineAnnealingWarmRestarts
from torchvision.models import resnet50, ResNet50_Weights
from tqdm import tqdm
import matplotlib.pyplot as plt
from PIL import Image
import numpy as np
import random
from dataclasses import dataclass
from typing import Optional
from collections import defaultdict


# Set seeds for reproducibility
def set_seed(seed=42):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


class FruitVegDataset(Dataset):
    def __init__(self, root_dir, split="train", transform=None):
        self.root_dir = os.path.join(root_dir, split)
        self.transform = transform
        self.classes = sorted(os.listdir(self.root_dir))
        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}

        self.images = []
        self.labels = []

        for class_name in self.classes:
            class_path = os.path.join(self.root_dir, class_name)
            for img_name in os.listdir(class_path):
                if img_name.lower().endswith((".png", ".jpg", ".jpeg")):
                    self.images.append(os.path.join(class_path, img_name))
                    self.labels.append(self.class_to_idx[class_name])

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        img_path = self.images[idx]
        label = self.labels[idx]

        image = Image.open(img_path).convert("RGB")
        if self.transform:
            image = self.transform(image)

        return image, label


@dataclass
class HyperparameterConfig:
    # === æ•°æ®ç›¸å…³è¶…å‚æ•° ===
    batch_size: int = 16
    num_workers: int = 4  # æ•°æ®åŠ è½½å¹¶è¡Œåº¦
    pin_memory: bool = True

    # === ä¼˜åŒ–å™¨ç›¸å…³è¶…å‚æ•° ===
    learning_rate: float = 1e-3  # ç¨å¾®æé«˜å­¦ä¹ ç‡ï¼ŒåŠ å¿«æ”¶æ•›
    weight_decay: float = 0.01  # L2æ­£åˆ™åŒ–
    betas: tuple = (0.9, 0.999)  # AdamWçš„momentumå‚æ•°
    eps: float = 1e-8  # AdamWçš„æ•°å€¼ç¨³å®šæ€§å‚æ•°

    # === å­¦ä¹ ç‡è°ƒåº¦å™¨è¶…å‚æ•° ===
    scheduler_type: str = "onecycle"  # onecycle, cosine, step, exponential
    max_lr: float = 1e-3  # OneCycleLRçš„æœ€å¤§å­¦ä¹ ç‡
    pct_start: float = 0.1  # å­¦ä¹ ç‡ä¸Šå‡é˜¶æ®µæ¯”ä¾‹
    anneal_strategy: str = "cos"  # é€€ç«ç­–ç•¥

    # === è®­ç»ƒç›¸å…³è¶…å‚æ•° ===
    epochs: int = 60  # å¢åŠ è®­ç»ƒè½®æ•°
    warmup_epochs: int = 5  # é¢„çƒ­é˜¶æ®µ
    patience: int = 10  # æ—©åœè€å¿ƒå€¼

    # === æ­£åˆ™åŒ–è¶…å‚æ•° ===
    dropout: float = 0.3  # Dropoutæ¦‚ç‡
    dropblock_drop_rate: float = 0.1  # DropBlockæ¦‚ç‡ï¼ˆå¦‚æœä½¿ç”¨ï¼‰
    label_smoothing: float = 0.1  # æ ‡ç­¾å¹³æ»‘

    # === æ··åˆç²¾åº¦è®­ç»ƒ ===
    use_amp: bool = True  # è‡ªåŠ¨æ··åˆç²¾åº¦

    # === æŒ‡æ•°ç§»åŠ¨å¹³å‡ ===
    use_ema: bool = False  # æ˜¯å¦ä½¿ç”¨EMA
    ema_decay: float = 0.9999  # EMAè¡°å‡ç‡

    # === æ¨¡å‹æ¶æ„è¶…å‚æ•° ===
    backbone: str = "resnet50"  # ä¸»å¹²ç½‘ç»œ
    pretrained: bool = True  # æ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
    freeze_backbone: bool = False  # æ˜¯å¦å†»ç»“ä¸»å¹²ç½‘ç»œ
    freeze_layers: int = 30  # å†»ç»“çš„å±‚æ•°ï¼ˆä»å‰å¾€åï¼‰

    # === æ•°æ®å¢å¼ºè¶…å‚æ•° ===
    mixup_alpha: float = 0.0  # MixUpå‚æ•°ï¼ˆè®¾ä¸º0ç¦ç”¨ï¼‰
    cutmix_alpha: float = 0.0  # CutMixå‚æ•°ï¼ˆè®¾ä¸º0ç¦ç”¨ï¼‰

    # === æŸå¤±å‡½æ•°è¶…å‚æ•° ===
    loss_function: str = "crossentropy"  # crossentropy, focal, etc.
    focal_alpha: float = 1.0  # Focal Lossçš„alphaå‚æ•°
    focal_gamma: float = 2.0  # Focal Lossçš„gammaå‚æ•°

    # === æ¢¯åº¦ç›¸å…³è¶…å‚æ•° ===
    gradient_clip_val: Optional[float] = 1.0  # æ¢¯åº¦è£å‰ª
    accumulate_grad_batches: int = 1  # æ¢¯åº¦ç´¯ç§¯


class EarlyStopping:
    """æ—©åœæœºåˆ¶"""

    def __init__(self, patience=10, min_delta=0.001, restore_best_weights=True):
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best_weights = restore_best_weights
        self.best_loss = float("inf")
        self.counter = 0
        self.best_weights = None

    def __call__(self, val_loss, model):
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            if self.restore_best_weights:
                self.best_weights = model.state_dict().copy()
        else:
            self.counter += 1

        if self.counter >= self.patience:
            if self.restore_best_weights and self.best_weights:
                model.load_state_dict(self.best_weights)
            return True
        return False


class ModelEMA:
    """æŒ‡æ•°ç§»åŠ¨å¹³å‡"""

    def __init__(self, model, decay=0.9999):
        self.model = model
        self.decay = decay
        self.shadow = {}
        self.backup = {}
        self.register()

    def register(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = param.data.clone()

    def update(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                assert name in self.shadow
                new_average = (
                    1.0 - self.decay
                ) * param.data + self.decay * self.shadow[name]
                self.shadow[name] = new_average.clone()

    def apply_shadow(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                assert name in self.shadow
                self.backup[name] = param.data
                param.data = self.shadow[name]

    def restore(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                assert name in self.backup
                param.data = self.backup[name]
        self.backup = {}


class LearningRateScheduler:
    """å­¦ä¹ ç‡è°ƒåº¦å™¨ç®¡ç†"""

    def __init__(self, optimizer, config, steps_per_epoch):
        self.optimizer = optimizer
        self.config = config
        self.steps_per_epoch = steps_per_epoch
        self.scheduler = self._create_scheduler()

    def _create_scheduler(self):
        if self.config.scheduler_type == "onecycle":
            return OneCycleLR(
                self.optimizer,
                max_lr=self.config.max_lr,
                epochs=self.config.epochs,
                steps_per_epoch=self.steps_per_epoch,
                pct_start=self.config.pct_start,
                anneal_strategy=self.config.anneal_strategy,
            )
        elif self.config.scheduler_type == "cosine":
            return CosineAnnealingWarmRestarts(
                self.optimizer, T_0=self.config.epochs // 4, T_mult=2
            )
        else:
            return None

    def step(self, epoch=None):
        if self.scheduler:
            if self.config.scheduler_type == "onecycle":
                self.scheduler.step()
            elif self.config.scheduler_type == "cosine":
                self.scheduler.step(epoch)

    def get_last_lr(self):
        if self.scheduler:
            return self.scheduler.get_last_lr()
        return [self.optimizer.param_groups[0]["lr"]]


class OptimizedTrainer:
    def __init__(self, model, train_loader, val_loader, config):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # åˆå§‹åŒ–ç»„ä»¶
        self.criterion = self._create_criterion()
        self.optimizer = self._create_optimizer()
        self.scheduler = LearningRateScheduler(
            self.optimizer, config, len(train_loader)
        )
        self.scaler = GradScaler() if config.use_amp else None
        self.early_stopping = EarlyStopping(patience=config.patience)
        self.ema = (
            ModelEMA(model, decay=config.ema_decay)
            if hasattr(config, "use_ema") and config.use_ema
            else None
        )

        # è®­ç»ƒå†å²
        self.history = defaultdict(list)
        self.best_val_acc = 0
        self.start_time = time.time()

    def _create_criterion(self):
        """åˆ›å»ºæŸå¤±å‡½æ•°"""
        if self.config.loss_function == "crossentropy":
            return nn.CrossEntropyLoss(label_smoothing=self.config.label_smoothing)
        elif self.config.loss_function == "focal":
            # è¿™é‡Œå¯ä»¥å®ç°Focal Loss
            return nn.CrossEntropyLoss(label_smoothing=self.config.label_smoothing)
        else:
            return nn.CrossEntropyLoss()

    def _create_optimizer(self):
        """åˆ›å»ºä¼˜åŒ–å™¨"""
        return optim.AdamW(
            self.model.parameters(),
            lr=self.config.learning_rate,
            weight_decay=self.config.weight_decay,
            betas=self.config.betas,
            eps=self.config.eps,
        )

    def _warmup_lr(self, epoch, warmup_epochs):
        """å­¦ä¹ ç‡é¢„çƒ­"""
        if epoch < warmup_epochs:
            lr_scale = (epoch + 1) / warmup_epochs
            for param_group in self.optimizer.param_groups:
                param_group["lr"] = self.config.learning_rate * lr_scale

    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        # é¢„çƒ­å­¦ä¹ ç‡
        if epoch < self.config.warmup_epochs:
            self._warmup_lr(epoch, self.config.warmup_epochs)

        pbar = tqdm(self.train_loader, desc=f"Epoch {epoch + 1}/{self.config.epochs}")

        for batch_idx, (inputs, labels) in enumerate(pbar):
            inputs, labels = inputs.to(self.device), labels.to(self.device)

            self.optimizer.zero_grad()

            # æ··åˆç²¾åº¦è®­ç»ƒ
            if self.config.use_amp and self.scaler:
                with autocast(device_type="cuda", dtype=torch.bfloat16):
                    outputs = self.model(inputs)
                    loss = self.criterion(outputs, labels)

                self.scaler.scale(loss).backward()

                # æ¢¯åº¦è£å‰ª
                if self.config.gradient_clip_val:
                    self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(), self.config.gradient_clip_val
                    )

                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                outputs = self.model(inputs)
                loss = self.criterion(outputs, labels)
                loss.backward()

                # æ¢¯åº¦è£å‰ª
                if self.config.gradient_clip_val:
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(), self.config.gradient_clip_val
                    )

                self.optimizer.step()

            # æ›´æ–°å­¦ä¹ ç‡
            if epoch >= self.config.warmup_epochs:
                self.scheduler.step()

            # æ›´æ–°EMA
            if self.ema:
                self.ema.update()

            # ç»Ÿè®¡
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

            # æ›´æ–°è¿›åº¦æ¡
            current_lr = self.scheduler.get_last_lr()[0]
            pbar.set_postfix(
                {
                    "loss": f"{loss.item():.4f}",
                    "acc": f"{100.0 * correct / total:.2f}%",
                    "lr": f"{current_lr:.6f}",
                }
            )

        train_loss = running_loss / len(self.train_loader)
        train_acc = 100.0 * correct / total

        return train_loss, train_acc

    def validate(self):
        """éªŒè¯"""

        if self.ema:
            self.ema.apply_shadow()

        self.model.eval()
        running_loss = 0.0
        correct = 0
        total = 0

        with torch.no_grad():
            for inputs, labels in tqdm(self.val_loader, desc="Validation"):
                inputs, labels = inputs.to(self.device), labels.to(self.device)

                if self.config.use_amp:
                    with autocast(device_type="cuda", dtype=torch.bfloat16):
                        outputs = self.model(inputs)
                        loss = self.criterion(outputs, labels)
                else:
                    outputs = self.model(inputs)
                    loss = self.criterion(outputs, labels)

                running_loss += loss.item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()

        val_loss = running_loss / len(self.val_loader)
        val_acc = 100.0 * correct / total

        if self.ema:
            self.ema.restore()

        return val_loss, val_acc

    def save_checkpoint(self, epoch, val_acc, is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            "epoch": epoch,
            "model_state_dict": self.model.state_dict(),
            "optimizer_state_dict": self.optimizer.state_dict(),
            "scheduler_state_dict": self.scheduler.scheduler.state_dict()
            if self.scheduler.scheduler
            else None,
            "val_acc": val_acc,
            # "config": self.config,
            "history": dict(self.history),
        }

        if self.ema:
            checkpoint["ema_state_dict"] = self.ema.shadow
            self.ema.apply_shadow()
            checkpoint["model_state_dict"] = self.model.state_dict()
            self.ema.restore()

        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        torch.save(checkpoint, "latest_checkpoint.pth")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            torch.save(checkpoint, "best_model.pth")
            print(f"ğŸ’¾ Saved best model with validation accuracy: {val_acc:.2f}%")

    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print("ğŸš€ Starting training with optimized hyperparameters...")
        print(f"ğŸ“Š Training on {len(self.train_loader.dataset)} samples")
        print(f"ğŸ“Š Validating on {len(self.val_loader.dataset)} samples")
        print(f"ğŸ”§ Batch size: {self.config.batch_size}")
        print(f"ğŸ”§ Learning rate: {self.config.learning_rate}")
        print(f"ğŸ”§ Weight decay: {self.config.weight_decay}")
        print(f"ğŸ”§ Epochs: {self.config.epochs}")
        print("-" * 60)

        for epoch in range(self.config.epochs):
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(epoch)

            # éªŒè¯
            val_loss, val_acc = self.validate()

            # è®°å½•å†å²
            self.history["train_loss"].append(train_loss)
            self.history["train_acc"].append(train_acc)
            self.history["val_loss"].append(val_loss)
            self.history["val_acc"].append(val_acc)
            self.history["lr"].append(self.scheduler.get_last_lr()[0])

            # æ‰“å°ç»“æœ
            elapsed_time = time.time() - self.start_time
            print(f"\nEpoch {epoch + 1}/{self.config.epochs} (â±ï¸ {elapsed_time:.1f}s):")
            print(f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%")
            print(f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%")
            print(f"Learning Rate: {self.scheduler.get_last_lr()[0]:.6f}")

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            is_best = val_acc > self.best_val_acc
            if is_best:
                self.best_val_acc = val_acc

            self.save_checkpoint(epoch, val_acc, is_best)

            # æ—©åœæ£€æŸ¥
            if self.early_stopping(val_loss, self.model):
                print(f"ğŸ›‘ Early stopping triggered at epoch {epoch + 1}")
                print(f"ğŸ¯ Best validation accuracy: {self.best_val_acc:.2f}%")
                break

        print("\nğŸ‰ Training completed!")
        print(f"ğŸ¯ Best validation accuracy: {self.best_val_acc:.2f}%")
        print(
            f"â±ï¸ Total training time: {(time.time() - self.start_time) / 60:.1f} minutes"
        )

        return dict(self.history)


class AttentionLayer(nn.Module):
    def __init__(self, in_features):
        super().__init__()
        self.attention = nn.Sequential(
            nn.Linear(in_features, in_features // 4),
            nn.ReLU(),
            nn.Linear(in_features // 4, in_features),
            nn.Sigmoid(),
        )

    def forward(self, x):
        weights = self.attention(x)
        return x * weights


class OptimizedCNN(nn.Module):
    def __init__(self, num_classes):
        super().__init__()

        self.backbone = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)

        freeze_ratio = 0.7
        total_layers = len(list(self.backbone.named_parameters()))
        freeze_count = int(total_layers * freeze_ratio)
        for i, (name, param) in enumerate(self.backbone.named_parameters()):
            if i < freeze_count:
                param.requires_grad = False
            else:
                param.requires_grad = True

        num_features = self.backbone.fc.in_features
        self.backbone.fc = nn.Sequential(
            nn.Linear(num_features, 1024),
            nn.LayerNorm(1024),
            nn.GELU(),
            nn.Dropout(0.4),
            AttentionLayer(1024),
            nn.Linear(1024, 512),
            nn.LayerNorm(512),
            nn.GELU(),
            nn.Dropout(0.3),
            nn.Linear(512, num_classes),
        )

    def forward(self, x):
        return self.backbone(x)


def plot_optimized_results(history):
    plt.style.use("seaborn-v0_8")
    plt.figure(figsize=(15, 5))

    # Plot Accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history["train_acc"], label="Training", marker="o")
    plt.plot(history["val_acc"], label="Validation", marker="o")
    plt.title("Model Accuracy with Optimizations")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy (%)")
    plt.legend()
    plt.grid(True)

    # Plot Loss
    plt.subplot(1, 2, 2)
    plt.plot(history["train_loss"], label="Training", marker="o")
    plt.plot(history["val_loss"], label="Validation", marker="o")
    plt.title("Model Loss with Optimizations")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.savefig("optimized_results.png", dpi=300, bbox_inches="tight")
    plt.show()

    # Print best metrics
    best_train_acc = max(history["train_acc"])
    best_val_acc = max(history["val_acc"])
    print(f"\nBest Training Accuracy: {best_train_acc:.2f}%")
    print(f"Best Validation Accuracy: {best_val_acc:.2f}%")
