# Fresco - æ°´æœè”¬èœåˆ†ç±»å™¨
**FR**uit & veg**E**table cla**S**sifi**C**ati**O**n

ä¸€ä¸ªåŸºäºæ·±åº¦å­¦ä¹ çš„æ°´æœè”¬èœå›¾åƒåˆ†ç±»é¡¹ç›®ï¼Œä½¿ç”¨ä¼˜åŒ–çš„ ResNet50 æ¶æ„å®ç°é«˜ç²¾åº¦åˆ†ç±»ã€‚

## ğŸ¯ é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®å®ç°äº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ°´æœè”¬èœå›¾åƒåˆ†ç±»ç³»ç»Ÿï¼Œæ”¯æŒ 36 ç§ä¸åŒç±»åˆ«çš„æ°´æœå’Œè”¬èœè¯†åˆ«ã€‚é¡¹ç›®é‡‡ç”¨äº†ç°ä»£åŒ–çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯æ ˆï¼ŒåŒ…æ‹¬æ•°æ®å¢å¼ºã€æ··åˆç²¾åº¦è®­ç»ƒã€å­¦ä¹ ç‡è°ƒåº¦ç­‰ä¼˜åŒ–ç­–ç•¥ã€‚

### ğŸ“Š æ•°æ®é›†ä¿¡æ¯
- **æ•°æ®æ¥æº**: [å·¥ä½œå° - Heywhale.com](https://www.heywhale.com/mw/dataset/676167f5e8187b578b8c17d1/file)
- **ç±»åˆ«æ•°é‡**: 36 ç§æ°´æœè”¬èœ
- **æ•°æ®åˆ†å‰²**: è®­ç»ƒé›†/éªŒè¯é›†/æµ‹è¯•é›†
- **æ”¯æŒç±»åˆ«**: 
  - æ°´æœï¼šè‹¹æœã€é¦™è•‰ã€è‘¡è„ã€çŒ•çŒ´æ¡ƒã€æŸ æª¬ã€èŠ’æœã€æ©™å­ã€æ¢¨ã€è èã€çŸ³æ¦´ã€è¥¿ç“œç­‰
  - è”¬èœï¼šç”œèœæ ¹ã€ç”œæ¤’ã€å·å¿ƒèœã€èƒ¡èåœã€èŠ±æ¤°èœã€è¾£æ¤’ã€ç‰ç±³ã€é»„ç“œã€èŒ„å­ã€å¤§è’œã€ç”Ÿå§œç­‰

## ğŸ—ï¸ é¡¹ç›®ç»“æ„

```
Fresco/
â”œâ”€â”€ train_latest.py          # ä¸»è®­ç»ƒè„šæœ¬
â”œâ”€â”€ utils_latest.py          # å·¥å…·å‡½æ•°å’Œæ¨¡å‹å®šä¹‰
â”œâ”€â”€ predict_latest.py        # é¢„æµ‹è„šæœ¬
â”œâ”€â”€ best_model.pth          # æœ€ä½³æ¨¡å‹æƒé‡
â”œâ”€â”€ latest_checkpoint.pth   # æœ€æ–°æ£€æŸ¥ç‚¹
â”œâ”€â”€ optimized_results.png   # è®­ç»ƒç»“æœå¯è§†åŒ–
â”œâ”€â”€ Dataset/                # æ•°æ®é›†ç›®å½•
â”‚   â”œâ”€â”€ train/              # è®­ç»ƒæ•°æ®
â”‚   â”œâ”€â”€ validation/         # éªŒè¯æ•°æ®
â”‚   â””â”€â”€ test/               # æµ‹è¯•æ•°æ®
â”œâ”€â”€ web/                    # Webåº”ç”¨
â”‚   â”œâ”€â”€ app.py              # Flaskåº”ç”¨
â”‚   â””â”€â”€ templates/          # ç½‘é¡µæ¨¡æ¿
â””â”€â”€ train_v*/               # å†å²ç‰ˆæœ¬è®­ç»ƒä»£ç 
```

## ğŸ”§ æŠ€æœ¯æ ˆ

- **æ·±åº¦å­¦ä¹ æ¡†æ¶**: PyTorch 2.0+
- **è®¡ç®—æœºè§†è§‰**: torchvision
- **æ¨¡å‹æ¶æ„**: ResNet50 (é¢„è®­ç»ƒ) + è‡ªå®šä¹‰åˆ†ç±»å¤´
- **ä¼˜åŒ–å™¨**: AdamW
- **å­¦ä¹ ç‡è°ƒåº¦**: OneCycleLR / CosineAnnealingWarmRestarts
- **æ•°æ®å¢å¼º**: torchvision.transforms.v2
- **æ··åˆç²¾åº¦è®­ç»ƒ**: torch.amp
- **å¯è§†åŒ–**: matplotlib
- **Webæ¡†æ¶**: Flask

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒå®‰è£…

```bash
# å…‹éš†é¡¹ç›®
git clone <repository-url>
cd Fresco

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### è®­ç»ƒæ¨¡å‹

```bash
python train_latest.py
```

### è¿›è¡Œé¢„æµ‹

```bash
python predict_latest.py
```

### å¯åŠ¨Webåº”ç”¨

```bash
cd web
python app.py
```

## ğŸ“‹ æ¨¡å‹è®­ç»ƒè¯¦ç»†æµç¨‹

### 1. æ•°æ®é¢„å¤„ç†ä¸å¢å¼º

#### è®­ç»ƒæ•°æ®å¢å¼ºç­–ç•¥ï¼š
```python
train_transform = v2.Compose([
    v2.Resize((256, 256)),                                    # å›¾åƒç¼©æ”¾
    v2.RandomResizedCrop(224, scale=(0.6, 1.0), ratio=(0.7, 1.3)),  # éšæœºè£å‰ª
    v2.RandomHorizontalFlip(p=0.5),                          # æ°´å¹³ç¿»è½¬
    v2.RandomVerticalFlip(p=0.4),                            # å‚ç›´ç¿»è½¬
    v2.RandomRotation(degrees=35),                           # éšæœºæ—‹è½¬
    v2.RandomAffine(degrees=0, translate=(0.2, 0.2), 
                    scale=(0.8, 1.2), shear=(-15, 15, -15, 15)),  # ä»¿å°„å˜æ¢
    v2.RandomPerspective(distortion_scale=0.2, p=0.3),       # é€è§†å˜æ¢
    v2.ColorJitter(brightness=0.4, contrast=0.4, 
                   saturation=0.4, hue=0.15),                # é¢œè‰²æŠ–åŠ¨
    v2.RandomAutocontrast(p=0.4),                            # è‡ªåŠ¨å¯¹æ¯”åº¦
    v2.RandomEqualize(p=0.3),                                # ç›´æ–¹å›¾å‡è¡¡åŒ–
    v2.RandomAdjustSharpness(sharpness_factor=2, p=0.3),     # é”åº¦è°ƒæ•´
    v2.RandomPosterize(bits=4, p=0.3),                       # è‰²å½©é‡åŒ–
    v2.RandomSolarize(threshold=128, p=0.3),                 # æ›å…‰æ•ˆæœ
    v2.RandomApply([v2.GaussianBlur(kernel_size=5, sigma=(0.1, 3.0))], p=0.4),  # é«˜æ–¯æ¨¡ç³Š
    v2.RandomErasing(p=0.3, scale=(0.02, 0.25), ratio=(0.3, 3.3)),  # éšæœºæ“¦é™¤
    v2.ToTensor(),                                           # è½¬ä¸ºå¼ é‡
    v2.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # ImageNetæ ‡å‡†åŒ–
])
```

#### éªŒè¯æ•°æ®é¢„å¤„ç†ï¼š
```python
val_transform = v2.Compose([
    v2.Resize((224, 224)),                                   # å›ºå®šå°ºå¯¸
    v2.ToTensor(),                                           # è½¬ä¸ºå¼ é‡
    v2.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # æ ‡å‡†åŒ–
])
```

### 2. æ¨¡å‹æ¶æ„è®¾è®¡

#### åŸºç¡€æ¶æ„ï¼š
- **ä¸»å¹²ç½‘ç»œ**: ResNet50 (ImageNeté¢„è®­ç»ƒæƒé‡)
- **å†»ç»“ç­–ç•¥**: å†»ç»“å‰70%çš„å±‚ï¼Œå¾®è°ƒå30%çš„å±‚
- **åˆ†ç±»å¤´**: å¤šå±‚æ„ŸçŸ¥æœº + æ³¨æ„åŠ›æœºåˆ¶

#### è‡ªå®šä¹‰åˆ†ç±»å¤´ï¼š
```python
self.backbone.fc = nn.Sequential(
    nn.Linear(num_features, 1024),      # ç¬¬ä¸€å±‚çº¿æ€§å˜æ¢
    nn.LayerNorm(1024),                 # å±‚å½’ä¸€åŒ–
    nn.GELU(),                          # GELUæ¿€æ´»å‡½æ•°
    nn.Dropout(0.4),                    # Dropoutæ­£åˆ™åŒ–
    AttentionLayer(1024),               # è‡ªæ³¨æ„åŠ›å±‚
    nn.Linear(1024, 512),               # ç¬¬äºŒå±‚çº¿æ€§å˜æ¢
    nn.LayerNorm(512),                  # å±‚å½’ä¸€åŒ–
    nn.GELU(),                          # GELUæ¿€æ´»å‡½æ•°
    nn.Dropout(0.3),                    # Dropoutæ­£åˆ™åŒ–
    nn.Linear(512, num_classes)         # è¾“å‡ºå±‚
)
```

#### æ³¨æ„åŠ›æœºåˆ¶ï¼š
```python
class AttentionLayer(nn.Module):
    def __init__(self, in_features):
        super().__init__()
        self.attention = nn.Sequential(
            nn.Linear(in_features, in_features // 4),  # é™ç»´
            nn.ReLU(),                                 # æ¿€æ´»
            nn.Linear(in_features // 4, in_features), # å‡ç»´
            nn.Sigmoid()                               # æ³¨æ„åŠ›æƒé‡
        )
    
    def forward(self, x):
        weights = self.attention(x)
        return x * weights  # åŠ æƒç‰¹å¾
```

### 3. è¶…å‚æ•°é…ç½®

```python
@dataclass
class HyperparameterConfig:
    # æ•°æ®ç›¸å…³
    batch_size: int = 16                    # æ‰¹å¤§å°
    num_workers: int = 4                    # æ•°æ®åŠ è½½å¹¶è¡Œåº¦
    pin_memory: bool = True                 # å†…å­˜é”å®š

    # ä¼˜åŒ–å™¨ç›¸å…³
    learning_rate: float = 1e-3             # åˆå§‹å­¦ä¹ ç‡
    weight_decay: float = 0.01              # æƒé‡è¡°å‡(L2æ­£åˆ™åŒ–)
    betas: tuple = (0.9, 0.999)             # AdamWåŠ¨é‡å‚æ•°
    eps: float = 1e-8                       # æ•°å€¼ç¨³å®šæ€§å‚æ•°

    # å­¦ä¹ ç‡è°ƒåº¦
    scheduler_type: str = "onecycle"        # è°ƒåº¦å™¨ç±»å‹
    max_lr: float = 1e-3                    # æœ€å¤§å­¦ä¹ ç‡
    pct_start: float = 0.1                  # å­¦ä¹ ç‡ä¸Šå‡é˜¶æ®µæ¯”ä¾‹
    anneal_strategy: str = "cos"            # é€€ç«ç­–ç•¥

    # è®­ç»ƒç›¸å…³
    epochs: int = 60                        # è®­ç»ƒè½®æ•°
    warmup_epochs: int = 5                  # é¢„çƒ­è½®æ•°
    patience: int = 10                      # æ—©åœè€å¿ƒå€¼

    # æ­£åˆ™åŒ–
    dropout: float = 0.3                    # Dropoutæ¦‚ç‡
    label_smoothing: float = 0.1            # æ ‡ç­¾å¹³æ»‘

    # æ··åˆç²¾åº¦è®­ç»ƒ
    use_amp: bool = True                    # å¯ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦

    # æ¢¯åº¦ç›¸å…³
    gradient_clip_val: float = 1.0          # æ¢¯åº¦è£å‰ª
    accumulate_grad_batches: int = 1        # æ¢¯åº¦ç´¯ç§¯
```

### 4. è®­ç»ƒä¼˜åŒ–ç­–ç•¥

#### å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥ï¼š
- **OneCycleLR**: å•å‘¨æœŸå­¦ä¹ ç‡è°ƒåº¦ï¼Œå¿«é€Ÿæ”¶æ•›
- **é¢„çƒ­é˜¶æ®µ**: å‰5ä¸ªepochçº¿æ€§å¢åŠ å­¦ä¹ ç‡
- **é€€ç«ç­–ç•¥**: ä½™å¼¦é€€ç«é™ä½å­¦ä¹ ç‡

#### æ­£åˆ™åŒ–æŠ€æœ¯ï¼š
- **æƒé‡è¡°å‡**: L2æ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ
- **Dropout**: éšæœºå¤±æ´»ç¥ç»å…ƒ
- **æ ‡ç­¾å¹³æ»‘**: å‡å°‘è¿‡åº¦è‡ªä¿¡é¢„æµ‹
- **æ¢¯åº¦è£å‰ª**: é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸

#### æ··åˆç²¾åº¦è®­ç»ƒï¼š
- ä½¿ç”¨ `torch.amp.autocast` å’Œ `GradScaler`
- åŠ é€Ÿè®­ç»ƒå¹¶å‡å°‘æ˜¾å­˜å ç”¨
- ä¿æŒæ•°å€¼ç¨³å®šæ€§

#### æ—©åœæœºåˆ¶ï¼š
- ç›‘æ§éªŒè¯æŸå¤±ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
- è€å¿ƒå€¼ä¸º10ä¸ªepoch
- è‡ªåŠ¨æ¢å¤æœ€ä½³æƒé‡

### 5. è®­ç»ƒæµç¨‹ç›‘æ§

#### å®æ—¶æŒ‡æ ‡è¿½è¸ªï¼š
- è®­ç»ƒ/éªŒè¯ æŸå¤±å’Œå‡†ç¡®ç‡
- å­¦ä¹ ç‡å˜åŒ–
- è®­ç»ƒæ—¶é—´ç»Ÿè®¡

#### æ¨¡å‹ä¿å­˜ç­–ç•¥ï¼š
- æ¯ä¸ªepochä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹ (`latest_checkpoint.pth`)
- éªŒè¯å‡†ç¡®ç‡æå‡æ—¶ä¿å­˜æœ€ä½³æ¨¡å‹ (`best_model.pth`)
- æ”¯æŒæ–­ç‚¹ç»­è®­

#### å¯è§†åŒ–è¾“å‡ºï¼š
- è®­ç»ƒè¿‡ç¨‹æ›²çº¿å›¾
- æŸå¤±å’Œå‡†ç¡®ç‡å˜åŒ–è¶‹åŠ¿
- è‡ªåŠ¨ä¿å­˜ä¸º `optimized_results.png`

## ğŸ“ˆ æ¨¡å‹æ€§èƒ½

é¡¹ç›®é‡‡ç”¨äº†å¤šç§ä¼˜åŒ–æŠ€æœ¯ç¡®ä¿æ¨¡å‹æ€§èƒ½ï¼š

1. **æ•°æ®å¢å¼º**: 15+ç§å¢å¼ºç­–ç•¥æå‡æ³›åŒ–èƒ½åŠ›
2. **è¿ç§»å­¦ä¹ **: ä½¿ç”¨ImageNeté¢„è®­ç»ƒæƒé‡
3. **æ¸è¿›å¼è§£å†»**: é€æ­¥è§£å†»ç½‘ç»œå±‚è¿›è¡Œå¾®è°ƒ
4. **æ³¨æ„åŠ›æœºåˆ¶**: å¢å¼ºç‰¹å¾è¡¨ç¤ºèƒ½åŠ›
5. **æ··åˆç²¾åº¦è®­ç»ƒ**: æå‡è®­ç»ƒæ•ˆç‡
6. **é«˜çº§ä¼˜åŒ–å™¨**: AdamW + OneCycleLRç»„åˆ

## ğŸ”® ä½¿ç”¨ç¤ºä¾‹

### è®­ç»ƒæ–°æ¨¡å‹
```python
from utils_latest import *
from torch.utils.data import DataLoader
from torchvision.transforms import v2

# é…ç½®è¶…å‚æ•°ï¼ˆç›´æ¥ä¿®æ”¹ HyperparameterConfig ç±»çš„é»˜è®¤å€¼ï¼‰
config = HyperparameterConfig()
config.epochs = 100                      # å¢åŠ è®­ç»ƒè½®æ•°
config.batch_size = 32                   # è°ƒæ•´æ‰¹å¤§å°
config.learning_rate = 2e-3              # æé«˜å­¦ä¹ ç‡
config.weight_decay = 0.02               # å¢åŠ æ­£åˆ™åŒ–
config.use_amp = True                    # å¯ç”¨æ··åˆç²¾åº¦
config.patience = 15                     # è°ƒæ•´æ—©åœç­–ç•¥

# è®¾ç½®è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# åˆ›å»ºæ•°æ®é›†å’ŒåŠ è½½å™¨ï¼ˆå‚è€ƒ train_latest.py ä¸­çš„å®Œæ•´æµç¨‹ï¼‰
# è¿™é‡Œå±•ç¤ºæ ¸å¿ƒæ­¥éª¤ï¼š
data_path = "/path/to/your/Dataset"
train_dataset = FruitVegDataset(data_path, "train", train_transform)
val_dataset = FruitVegDataset(data_path, "validation", val_transform)

train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)

# åˆ›å»ºæ¨¡å‹å’Œè®­ç»ƒå™¨
model = OptimizedCNN(num_classes=len(train_dataset.classes)).to(device)
trainer = OptimizedTrainer(model, train_loader, val_loader, config)

# å¼€å§‹è®­ç»ƒ
history = trainer.train()

# ç»˜åˆ¶ç»“æœ
plot_optimized_results(history)
```

### è¿›è¡Œé¢„æµ‹
```python
from utils_latest import OptimizedCNN
import torch

# åŠ è½½æ¨¡å‹
model = OptimizedCNN(num_classes=36)
checkpoint = torch.load('best_model.pth')
model.load_state_dict(checkpoint['model_state_dict'])

# é¢„æµ‹å›¾åƒ
predict_image('image_url_or_path', model)
```

## âš™ï¸ è¶…å‚æ•°é…ç½®è¯´æ˜

é¡¹ç›®çš„æ‰€æœ‰è¶…å‚æ•°éƒ½åœ¨ `utils_latest.py` æ–‡ä»¶ä¸­çš„ `HyperparameterConfig` ç±»ä¸­å®šä¹‰ã€‚ä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è°ƒæ•´é…ç½®ï¼š

### ğŸ”§ æ ¸å¿ƒè®­ç»ƒå‚æ•°
```python
config = HyperparameterConfig()

# åŸºç¡€è®­ç»ƒå‚æ•°
config.epochs = 60                       # è®­ç»ƒè½®æ•°
config.batch_size = 16                   # æ‰¹å¤§å°
config.learning_rate = 1e-3              # åˆå§‹å­¦ä¹ ç‡
config.weight_decay = 0.01               # æƒé‡è¡°å‡(L2æ­£åˆ™åŒ–)

# å­¦ä¹ ç‡è°ƒåº¦
config.scheduler_type = "onecycle"       # å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹
config.max_lr = 1e-3                     # æœ€å¤§å­¦ä¹ ç‡
config.warmup_epochs = 5                 # é¢„çƒ­è½®æ•°
config.patience = 10                     # æ—©åœè€å¿ƒå€¼
```

### ğŸ¯ ä¼˜åŒ–ç­–ç•¥å‚æ•°
```python
# æ­£åˆ™åŒ–è®¾ç½®
config.dropout = 0.3                     # Dropoutæ¦‚ç‡
config.label_smoothing = 0.1             # æ ‡ç­¾å¹³æ»‘
config.gradient_clip_val = 1.0           # æ¢¯åº¦è£å‰ª

# è®­ç»ƒåŠ é€Ÿ
config.use_amp = True                    # æ··åˆç²¾åº¦è®­ç»ƒ
config.num_workers = 4                   # æ•°æ®åŠ è½½çº¿ç¨‹æ•°
config.pin_memory = True                 # å†…å­˜é”å®š

# æ¨¡å‹æ¶æ„
config.backbone = "resnet50"             # ä¸»å¹²ç½‘ç»œ
config.pretrained = True                 # ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
```

### ğŸ“Š å¸¸ç”¨é…ç½®ç»„åˆ

#### é«˜æ€§èƒ½é…ç½®ï¼ˆå¼ºåŠ›GPUï¼‰ï¼š
```python
config.epochs = 100
config.batch_size = 32
config.learning_rate = 2e-3
config.num_workers = 8
config.use_amp = True
```

#### ä½èµ„æºé…ç½®ï¼ˆè¾ƒå¼±è®¾å¤‡ï¼‰ï¼š
```python
config.epochs = 40
config.batch_size = 8
config.learning_rate = 5e-4
config.num_workers = 2
config.use_amp = False
```

#### è°ƒè¯•é…ç½®ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰ï¼š
```python
config.epochs = 5
config.batch_size = 4
config.patience = 2
config.warmup_epochs = 1
```

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿æäº¤ Issue å’Œ Pull Request æ¥æ”¹è¿›é¡¹ç›®ï¼

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ã€‚